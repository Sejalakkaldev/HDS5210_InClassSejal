{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgeSWifbMrfN"
      },
      "source": [
        "# Week 11 Assignment\n",
        "\n",
        "\n",
        "Please do the programming exercise and verify that your code works using the tests, then think about your final project and fill out the questions in the second part.\n",
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3O5ZH-FMrfP"
      },
      "source": [
        "### 47.1: Filtering and summarizing data\n",
        "\n",
        "For this work, you'll find a data file in `/data/complications_all.csv`.\n",
        "\n",
        "Read in the data file and create a variable called `mo_hospitals` that contains a data frame from the `complications_all.csv` file, filtered down to only contain those hospitals from the state of Missouri (MO).\n",
        "\n",
        "Then aggregate that data by hospital into a variable named `mo_summary`.  There are some key fields that we want to summarize:\n",
        "* We want to know the earliest date that each hospital was participating in any program\n",
        "* We want to know the latest date that each hospital stopped participating in any program\n",
        "* We want to know the total number of patients in the denominators of these programs\n",
        "\n",
        "Some things to note:\n",
        "* You will need to convert the `Start Date` and `End Date` to actual datetime fields\n",
        "* You will need to clean up and convert the `Denominator` field to just be numeric - the rule that you should use it to simply remove any records where the `Denominator` is `'Not Available'`\n",
        "\n",
        "\n",
        "The final result of this step should be a new data frame called `mo_summary` that contains one row for each hospital and contains the min start date, max end date, and total denominator.  Use the names `start_date`, `end_date`, and `number` for those columns in `mo_summary`.\n",
        "\n",
        "\n",
        "You do not need to create your code in the form of a function, just make sure your variable names match what I've described above so the tests work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XgnSYw0kMrfQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# This is just to show you the name to use for the variable you need to create for this step to pass.\n",
        "all_hospitals = pd.read_csv('https://hds5210-data.s3.amazonaws.com/complications_all.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vO5423kfMrfR"
      },
      "outputs": [],
      "source": [
        "# Do you work here and in as many cells as you need to create a variable called `mo_summary` that matches the requirements\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV data\n",
        "data = pd.read_csv('https://hds5210-data.s3.amazonaws.com/complications_all.csv')\n",
        "\n",
        "# Filter data for hospitals in Missouri (MO)\n",
        "mo_hospitals = data[data['State'] == 'MO'].copy()\n",
        "\n",
        "# Convert 'Start Date' and 'End Date' columns to datetime\n",
        "mo_hospitals['Start Date'] = pd.to_datetime(mo_hospitals['Start Date'], errors='coerce')\n",
        "mo_hospitals['End Date'] = pd.to_datetime(mo_hospitals['End Date'], errors='coerce')\n",
        "\n",
        "# Remove rows where Denominator is 'Not Available' and convert it to numeric\n",
        "mo_hospitals = mo_hospitals[mo_hospitals['Denominator'] != 'Not Available']\n",
        "mo_hospitals['Denominator'] = pd.to_numeric(mo_hospitals['Denominator'], errors='coerce')\n",
        "\n",
        "# Aggregate the data by hospital\n",
        "mo_summary = mo_hospitals.groupby('Facility Name').agg(\n",
        "    start_date=('Start Date', 'min'),\n",
        "    end_date=('End Date', 'max'),\n",
        "    number=('Denominator', 'sum')\n",
        ").reset_index()\n",
        "\n",
        "# Set Facility Name as index to support .loc for assertions\n",
        "mo_summary.set_index('Facility Name', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ikVlT97EMrfS"
      },
      "outputs": [],
      "source": [
        "assert(mo_summary['number'].sum() == 1766908)\n",
        "assert(mo_summary['start_date'].min() == pd.Timestamp(2015,4,1))\n",
        "assert(mo_summary['end_date'].max() == pd.Timestamp(2018,6,30))\n",
        "assert(mo_summary.shape == (108,3))\n",
        "assert(mo_summary.loc['BARNES JEWISH HOSPITAL'].number == 131313)\n",
        "assert(mo_summary.loc['BOONE HOSPITAL CENTER'].number == 63099)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFxvKg0tMrfS"
      },
      "source": [
        "---\n",
        "\n",
        "### 47.2 Planning your final project\n",
        "\n",
        "You should be thinking about the things we've been learning and how you can apply them to your final project.  Use the rubric to help guid your thinking and then answer the questions below.  This is meant as a guide to help you think through what you will do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6rarD-gMrfT"
      },
      "source": [
        "#### A) Data Access\n",
        "\n",
        "Your project should include data from at least three distinct types of sources.  For example: AWS S3, Relational Databases, Internet, Web Services, local files.  List what data sources you're planning to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCT20defMrfT"
      },
      "source": [
        "**Double-click to enter your answer**\n",
        "\n",
        "For my final project, I plan to use:\n",
        "\n",
        "1. **AWS S3** for storing very large datasets, such as historical transaction records.\n",
        "\n",
        "2. **Relational database(s)**, such as PostgreSQL, for managing structured data that is queried often, like customer and transaction data.\n",
        "\n",
        "3. **Web APIs** to pull live data, say, on financial metrics or weather data.\n",
        "\n",
        "These will serve to create a dynamic environment for data, including a blend of static, structured, and dynamic data types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5IUWL46MrfT"
      },
      "source": [
        "#### B. Data Formats\n",
        "\n",
        "Your project should include data that comes in different file formats.  For example: HL7, EDI, HTML, CSV, Excel, JSON, XML.  List what data formats you're planning to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SAsLAIzMrfU"
      },
      "source": [
        "**Double-click to enter your answer**\n",
        "\n",
        "I expect the final deliverable will use the following data sources:\n",
        "1. **AWS S3**: Essentially, all kinds of historical transaction records.\n",
        "2. **Relational Database** (e.g. PostgreSQL), which can manage structured data such as transaction data and customer information. It is mainly used for data demands that need to be queried frequently and deeply.\n",
        "3. **Web APIs**: To fetch real-time external data, such as weather data, stock/future options data from IEXCloud and Apollo or other entity search tools, etc.\n",
        "\n",
        "These data sources are not only static form, but the back-end system also needs to support some kind of stateless structure. Therefore, in order to make the data environment more flexible and to cover more data formats, the following data sources will be used:\n",
        "I plan to use the following types in my final product:\n",
        "\n",
        "1. **CSV**: Store structured, tabular data, such as past transaction recording and customer data. The process of integrating and working with these files is simple.\n",
        "2. **JSON**: For semi-structured data, which may include data sources accessed through Web APIs over the Internet. This is a modern and efficient way to present the data. It can be nested.\n",
        "3. **Excel (XLSX)**: Business analysts or users who are more accustomed to Excel can save any data that they have manually. It is also very helpful to share with non-technical people.\n",
        "\n",
        "Combined these data formats, there are structured and semi-structured data forms, providing different data sources for developers in different technical front-end or background to be productive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puxT-cZZMrfU"
      },
      "source": [
        "#### C. Objective\n",
        "\n",
        "What purpose would your project serve in a real work setting?  Take a couple of paragraphs to write down why this is an interesting product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL50cd58MrfU"
      },
      "source": [
        "**Double-click to enter your answer**\n",
        "\n",
        "At a professional level, the project proposes an investment in a powerful data integration and analysis solution that unites them into a distinctive analytic backbone. Organizations tend to find it hard to analyse data because the mission of the organization already splits those into: cloud storage, relational databases, and external APIs. This project solves the above-last one by providing a unified repository where data is kept in a clean and structured manner to enable thorough analysis and reporting in near real time. This will help forecast accuracy, operational efficiency, and alignment with key performance metrics.\n",
        "\n",
        "The project is captivating because of its robustness with multiple data formats like CSV, JSON, Excel, etc., and would be used from various industries with different data demands-finance or healthcare. Combining automated API data retrieval with scheduled methods of batch processing from AWS S3 allows for a truly dynamic solution responding with an ebb and flow to metadata changes through time. This would eliminate manual data management, facilitating easy access, and empowering data-driven culture and decision-making within the organization, resulting in significant increases in both productivity and responsive strategy for the organization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-0uT_NyMrfU"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Submit your work via GitHub as normal\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}